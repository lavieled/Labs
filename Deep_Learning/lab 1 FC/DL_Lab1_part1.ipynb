{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning Lab: ECE-00450107\n",
    "## Meeting 1 - Part 1: Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before running the code in this file, make sure that you are **activating the enviourment** in which the following packages are installed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Definitions and Imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################\n",
    "## DO NOT EDIT THIS CODE SECTION\n",
    "from DL_Lab1_functions import *\n",
    "warnings.filterwarnings('ignore')\n",
    "os.environ[\"CUBLAS_WORKSPACE_CONFIG\"] = \":4096:8\"\n",
    "matplotlib.use('Agg')\n",
    "\n",
    "%matplotlib tk\n",
    "####################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set fixed seeds to enable reproducing the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################\n",
    "## DO NOT EDIT THIS CODE SECTION\n",
    "seed = 50 # age of SIPL :-)\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "torch.use_deterministic_algorithms(True)\n",
    "torch.backends.cuda.matmul.allow_tf32 = False\n",
    "torch.backends.cudnn.benchmark = False\n",
    "train_flag = False\n",
    "####################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example for a Fully Connected Neural Classifier\n",
    "\n",
    "Here we will build and train from scratch a fully-connected neural classifier on the MNIST  dataset.\n",
    "<center width=\"100%\"><img src=\"./assets/mnist.jpeg\" width=\"300px\"></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check if computer uses CPU or GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"This files runs with {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load MNIST dataset and get to know it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The train set contains 60000 images, divided into 10 sections:\n",
      "Label 5: 5421 examples\n",
      "Label 0: 5923 examples\n",
      "Label 4: 5842 examples\n",
      "Label 1: 6742 examples\n",
      "Label 9: 5949 examples\n",
      "Label 2: 5958 examples\n",
      "Label 3: 6131 examples\n",
      "Label 6: 5918 examples\n",
      "Label 7: 6265 examples\n",
      "Label 8: 5851 examples\n",
      "The test set contains 10000 images, divided into 10 sections:\n",
      "Label 7: 1028 examples\n",
      "Label 2: 1032 examples\n",
      "Label 1: 1135 examples\n",
      "Label 0: 980 examples\n",
      "Label 4: 982 examples\n",
      "Label 9: 1009 examples\n",
      "Label 5: 892 examples\n",
      "Label 6: 958 examples\n",
      "Label 3: 1010 examples\n",
      "Label 8: 974 examples\n"
     ]
    }
   ],
   "source": [
    "train = torchvision.datasets.MNIST(root=\"/usr/share/DL_exp/datasets/mnist\", train=True, download=True)\n",
    "test = torchvision.datasets.MNIST(root=\"/usr/share/DL_exp/datasets/mnist\", train=False, download=True)\n",
    "train_size = len(train)\n",
    "test_size = len(test)\n",
    "\n",
    "# Count the number of samples in each category\n",
    "print(f\"The train set contains {train_size} images, divided into 10 sections:\")\n",
    "labels = [label for _, label in train]\n",
    "label_counts = Counter(labels)\n",
    "for label, count in label_counts.items():\n",
    "    print(f\"Label {label}: {count} examples\")\n",
    "\n",
    "print(f\"The test set contains {test_size} images, divided into 10 sections:\")\n",
    "labels = [label for _, label in test]\n",
    "label_counts = Counter(labels)\n",
    "for label, count in label_counts.items():\n",
    "    print(f\"Label {label}: {count} examples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save data in tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train.data.to(torch.float).to(device)\n",
    "train_labels = train.targets.to(device)\n",
    "test_data = test.data.to(torch.float).to(device)\n",
    "test_labels = test.targets.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalize the images in the training dataset (standardize them) using the well known formula:\n",
    "$$Z = \\frac{X-\\mu_X}{\\sigma_X}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the mean and the std of the train set\n",
    "train_mean = torch.mean(train_data)\n",
    "train_std = torch.std(train_data)\n",
    "\n",
    "# normalize the train and test sets using the above mean and std\n",
    "train_data = (train_data-train_mean)/train_std\n",
    "test_data = (test_data-train_mean)/train_std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the class of the Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a Fully Connected Neural Network Architeture Class \n",
    "class OurNetwork(nn.Module):\n",
    "    def __init__(self, input_size: int, hidden_layer_size: int, output_size: int):\n",
    "        super(OurNetwork, self).__init__()\n",
    "        # define the network fully connected layers\n",
    "        self.fc_layer1 = nn.Linear(input_size, hidden_layer_size)\n",
    "        self.fc_layer2 = nn.Linear(hidden_layer_size, output_size)\n",
    "        # define a flatten layer\n",
    "        self.flatten = nn.Flatten()\n",
    "        # define a sigmoid (activation) layer\n",
    "        self.activation = nn.Sigmoid()\n",
    "      \n",
    "    def forward(self, x):\n",
    "        # define the input layer, operating on flattaned inputs\n",
    "        flattened_x = self.flatten(x)\n",
    "        # define the first layer, using linear operation and then activation\n",
    "        z1 = self.fc_layer1(flattened_x)\n",
    "        z2 = self.activation(z1)\n",
    "        # define the output layer\n",
    "        return self.fc_layer2(z2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define some hyper-parameters that will be used during the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining hyper-parameters\n",
    "hparams = Hyper_Params()\n",
    "hparams.train_size = train_size\n",
    "hparams.lr = 10e-2 \n",
    "hparams.batch_size = 40\n",
    "hparams.epochs = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up the Training Parameters: Use Gradient Descend as optimizer and Cross Entropy for loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the network model with a hidden layer of size 200 and send to device\n",
    "model = OurNetwork(input_size=784, hidden_layer_size=200, output_size=10).to(device)\n",
    "\n",
    "# Define the optimizer\n",
    "optimizer = optim.SGD(model.parameters(), lr=hparams.lr)\n",
    "# Define the loss criterion\n",
    "loss_function = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training started..., batch_size=500\n",
      "| Epoch No. | Iter No. | Train Loss | Train Accuracy | Test Loss | Test Accuracy |\n",
      "----------------------------------------------------------------------------------\n",
      "|     0     |    1     |    2.47    |      0.05      |   2.31    |     0.11      |\n",
      "----------------------------------------------------------------------------------\n",
      "|     0     |   1500   |    0.41    |      0.89      |   0.24    |     0.93      |\n",
      "----------------------------------------------------------------------------------\n",
      "|     1     |   1500   |    0.21    |      0.94      |   0.18    |     0.95      |\n",
      "----------------------------------------------------------------------------------\n",
      "|     2     |   1500   |    0.16    |      0.95      |   0.15    |     0.95      |\n",
      "----------------------------------------------------------------------------------\n",
      "|     3     |   1500   |    0.13    |      0.96      |   0.12    |     0.96      |\n",
      "----------------------------------------------------------------------------------\n",
      "|     4     |   1500   |    0.11    |      0.97      |   0.11    |     0.97      |\n",
      "----------------------------------------------------------------------------------\n",
      "|     5     |   1500   |    0.09    |      0.97      |    0.1    |     0.97      |\n",
      "----------------------------------------------------------------------------------\n",
      "|     6     |   1500   |    0.08    |      0.98      |   0.09    |     0.97      |\n",
      "----------------------------------------------------------------------------------\n",
      "|     7     |   1500   |    0.07    |      0.98      |   0.09    |     0.97      |\n",
      "----------------------------------------------------------------------------------\n",
      "|     8     |   1500   |    0.06    |      0.98      |   0.08    |     0.97      |\n",
      "----------------------------------------------------------------------------------\n",
      "|     9     |   1500   |    0.06    |      0.99      |   0.08    |     0.98      |\n",
      "----------------------------------------------------------------------------------\n",
      "|    10     |   1500   |    0.05    |      0.99      |   0.07    |     0.98      |\n",
      "----------------------------------------------------------------------------------\n",
      "|    11     |   1500   |    0.05    |      0.99      |   0.07    |     0.98      |\n",
      "----------------------------------------------------------------------------------\n",
      "|    12     |   1500   |    0.04    |      0.99      |   0.07    |     0.98      |\n",
      "----------------------------------------------------------------------------------\n",
      "|    13     |   1500   |    0.04    |      0.99      |   0.07    |     0.98      |\n",
      "----------------------------------------------------------------------------------\n",
      "|    14     |   1500   |    0.03    |      0.99      |   0.07    |     0.98      |\n",
      "----------------------------------------------------------------------------------\n",
      "|    15     |   1500   |    0.03    |      0.99      |   0.07    |     0.98      |\n",
      "----------------------------------------------------------------------------------\n",
      "|    16     |   1500   |    0.03    |      0.99      |   0.06    |     0.98      |\n",
      "----------------------------------------------------------------------------------\n",
      "|    17     |   1500   |    0.03    |      1.0       |   0.06    |     0.98      |\n",
      "----------------------------------------------------------------------------------\n",
      "|    18     |   1500   |    0.02    |      1.0       |   0.06    |     0.98      |\n",
      "----------------------------------------------------------------------------------\n",
      "|    19     |   1500   |    0.02    |      1.0       |   0.06    |     0.98      |\n",
      "----------------------------------------------------------------------------------\n",
      "Total training took 19.38 seconds\n",
      "Training finished.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if not train_flag:\n",
    "    print(\"Training started..., batch_size=500\")\n",
    "    train_flag = True\n",
    "    \n",
    "    # Start a progress graph and a performance table, for visualization of the trainig process:\n",
    "    hparams.fig, (hparams.ax1, hparams.ax2) = plt.subplots(2, 1, figsize=(15, 9))\n",
    "    print_performance_grid(Flag=True)\n",
    "    # Calculate how many iterations the model trains in each epoch\n",
    "    iter_num = int(np.ceil(hparams.train_size/hparams.batch_size))\n",
    "    \n",
    "    # Set the model to training mode\n",
    "    model.train()\n",
    "    start_time = time.time() # time the start of training\n",
    "    # Training loop:\n",
    "    for epoch in range(hparams.epochs):\n",
    "        # for each epoch, do:\n",
    "        hparams.epoch_accuracy_train = np.zeros(iter_num)\n",
    "        hparams.epoch_loss_train = np.zeros(iter_num)\n",
    "        # randomly reshuffle the training and test groups before each new epoch:\n",
    "        index = torch.randperm(hparams.train_size)\n",
    "        train_data_perm = train_data[index]\n",
    "        train_labels_perm = train_labels[index]\n",
    "        # for each batch, do:\n",
    "        for i, batch in enumerate(range(0, hparams.train_size, hparams.batch_size)):\n",
    "            # Get a new batch of images and labels\n",
    "            data = train_data_perm[batch:batch+hparams.batch_size].to(device) \n",
    "            target = train_labels_perm[batch:batch+hparams.batch_size].squeeze().to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            output = model(data) # Apply the network on the new examples\n",
    "            loss = loss_function(output, target) # calculate the value of the loss function\n",
    "            \n",
    "            # Backward pass - ALWAYS IN THIS ORDER!\n",
    "            optimizer.zero_grad() # First, delete the gradients from the previous iteration\n",
    "            loss.backward() # Run backward pass on the loss\n",
    "            optimizer.step() # Preform an algorithm step (using the optimizer)\n",
    "    \n",
    "            # Save the loss and accuracy for the graph visualization\n",
    "            hparams.epoch_accuracy_train[i] = multi_class_accuracy(output, target.squeeze().to(device))\n",
    "            hparams.epoch_loss_train[i] = loss.item()\n",
    "            if(i == 0 and epoch == 0) or ((i+1) == iter_num):\n",
    "                # Freeze the model in order to evaluate the loss and accuracy on the test set\n",
    "                model.eval()\n",
    "                test_out = model(test_data)\n",
    "                test_loss = loss_function(test_out, test_labels.squeeze()).item()\n",
    "                test_accuracy = multi_class_accuracy(test_out, test_labels.squeeze())\n",
    "                print_performance(epoch, i, hparams, test_loss, test_accuracy)\n",
    "                model.train()\n",
    "                          \n",
    "    plt.show()\n",
    "    print(f\"Total training took {time.time() - start_time:.2f} seconds\")\n",
    "    \n",
    "    print(\"Training finished.\")\n",
    "else:\n",
    "    print(\"Error: Please restart the kernel before running the train again.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:deep_learn]",
   "language": "python",
   "name": "conda-env-deep_learn-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
