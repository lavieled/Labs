{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "81dbe7e8-2368-43cb-9d0f-ebb6552a6392",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import string\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "447af7a0",
   "metadata": {},
   "source": [
    "Copy here the definition of the 'LetterClassifier' class that you defined in 'DL_Lab1_part3.ipynb'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "92edbb1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a Fully Connected Neural Network Architeture Class \n",
    "class LetterClassifier(nn.Module):\n",
    "    def __init__(self, input_size: int, hidden_layer_size: int, output_size: int):\n",
    "        super(LetterClassifier, self).__init__()\n",
    "        # define the network fully connected layers\n",
    "        self.fc_layer1 = nn.Linear(input_size, 392)\n",
    "        self.fc_layer2 = nn.Linear(392, 196)\n",
    "        self.fc_layer3 = nn.Linear(196, output_size)\n",
    "        # define a flatten layer\n",
    "        self.flatten = nn.Flatten()\n",
    "        # define a sigmoid (activation) layer\n",
    "        self.activation = nn.Sigmoid()\n",
    "      \n",
    "    def forward(self, x):\n",
    "        # define the input layer, operating on flattaned inputs\n",
    "        flattened_x = self.flatten(x)\n",
    "        # define the first layer, using linear operation and then activation\n",
    "        z1 = self.fc_layer1(flattened_x)\n",
    "        z2 = self.activation(z1)\n",
    "        z2 = self.fc_layer2(z2)\n",
    "        z3 = self.activation(z2)\n",
    "        # define the output layer\n",
    "        return self.fc_layer3(z3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fdb6c14",
   "metadata": {},
   "source": [
    "Complete the model definition (the call to 'LetterClassifier') with the layer sizes you used for your network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "97521071-cab3-48eb-9482-c0eee1ef47d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_classify(code_file, train_mean, train_std, model_path=\"letter_classifier.pth\"):\n",
    "\n",
    "    # Check if model file exists\n",
    "    if not os.path.exists(model_path):\n",
    "        print(f\"Error: Model file {model_path} not found!\")\n",
    "        print(\"Please make sure you have trained and saved your model first.\")\n",
    "        return None\n",
    "\n",
    "    # model file exists, load model\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    model = LetterClassifier(input_size=784, hidden_layer_size=392, output_size=26).to(device)\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    \n",
    "    # Load the saved tensor\n",
    "    test_images = torch.load(code_file)\n",
    "    \n",
    "    # Move to the correct device\n",
    "    test_images = test_images.to(device)\n",
    "         \n",
    "    # Normalize the test images\n",
    "    test_images = (test_images - train_mean) / train_std\n",
    "    \n",
    "    # Set model to evaluation mode\n",
    "    model.eval()\n",
    "    \n",
    "    # Get predictions\n",
    "    with torch.no_grad():\n",
    "        outputs = model(test_images)\n",
    "        _, predicted_indices = torch.max(outputs, 1)\n",
    "    \n",
    "    # Convert class indices back to letters (0->A, 1->B, etc.)\n",
    "    predicted_letters = [chr(idx.item() + ord('A')) for idx in predicted_indices]\n",
    "    predicted_code = ''.join(predicted_letters)\n",
    "    \n",
    "    print(f\"Predicted code: {predicted_code}\")\n",
    "\n",
    "    if 0:\n",
    "        # Visualize with predictions\n",
    "        fig, axes = plt.subplots(1, 5, figsize=(15, 3))\n",
    "        for i, img in enumerate(test_images.cpu()):\n",
    "            # Transpose the image before displaying\n",
    "            img_display = img.numpy().T\n",
    "            axes[i].imshow(img_display, cmap='gray')\n",
    "            axes[i].set_title(f\"Predicted: {predicted_letters[i]}\")\n",
    "            axes[i].axis('off')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    return predicted_code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b090b97b",
   "metadata": {},
   "source": [
    "Copy here the normalization values that were saved in 'norm_values.txt', and the name of the 'bin' file that holds the code's data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "947d40ed-9e0f-4c1a-b2f0-6a3befb88ed4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted code: TANSU\n",
      "Try using TANSU and see if you solved the challenge!\n"
     ]
    }
   ],
   "source": [
    "# complete the code file according to your chose :-)\n",
    "# red_lock_code.bin / black_lock_code.bin / blue_lock_code.bin / grey_lock_code.bin\n",
    "code_file = \"black_lock_code.bin\"\n",
    "mean_value = 43.92\n",
    "std_value = 84.39\n",
    "model_file = \"letter_classifier.pth\"\n",
    "if os.path.exists(code_file) and os.path.exists(model_file):\n",
    "    decoded_code = load_and_classify(code_file, float(mean_value), float(std_value), model_file)\n",
    "    if decoded_code:\n",
    "        print(f\"Try using {decoded_code.upper()} and see if you solved the challenge!\")\n",
    "else:\n",
    "    print(f\"File {code_file} or {model_file} not found!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
