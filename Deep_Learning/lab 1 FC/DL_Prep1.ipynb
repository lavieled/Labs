{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning Lab: ECE-00450107\n",
    "## Preparation for Meeting 1: Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before running the code in this file, make sure that you are **activating the enviourment** in which the following packages are installed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Definitions and Imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################\n",
    "## DO NOT EDIT THIS CODE SECTION\n",
    "from DL_Lab1_functions import *\n",
    "warnings.filterwarnings('ignore')\n",
    "os.environ[\"CUBLAS_WORKSPACE_CONFIG\"] = \":4096:8\"\n",
    "matplotlib.use('Agg')\n",
    "\n",
    "%matplotlib tk\n",
    "####################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set fixed seeds to enable reproducing the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################\n",
    "## DO NOT EDIT THIS CODE SECTION\n",
    "seed = 50 # age of SIPL :-)\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "torch.use_deterministic_algorithms(True)\n",
    "torch.backends.cuda.matmul.allow_tf32 = False\n",
    "torch.backends.cudnn.benchmark = False\n",
    "####################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example for a Fully Connected Neural Classifier\n",
    "\n",
    "Here we will build and train from scratch a fully-connected neural classifier on the MNIST  dataset.\n",
    "<center width=\"100%\"><img src=\"./assets/mnist.jpeg\" width=\"300px\"></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check if computer uses CPU or GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load MNIST dataset and get to know it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = torchvision.datasets.MNIST(root=\"/usr/share/DL_exp/datasets/mnist\", train=True, download=True)\n",
    "test = torchvision.datasets.MNIST(root=\"/usr/share/DL_exp/datasets/mnist\", train=False, download=True)\n",
    "train_size = len(train)\n",
    "test_size = len(test)\n",
    "\n",
    "# Display a few samples\n",
    "display_Mnist_Sample(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save data in tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train.data.to(torch.float).to(device)\n",
    "train_labels = train.targets.to(device)\n",
    "test_data = test.data.to(torch.float).to(device)\n",
    "test_labels = test.targets.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalize the images in the training dataset (standardize them) using the well known formula:\n",
    "$$Z = \\frac{X-\\mu_X}{\\sigma_X}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the mean and the std of the train set\n",
    "train_mean = torch.mean(train_data)\n",
    "train_std = torch.std(train_data)\n",
    "\n",
    "# normalize the train and test sets using the above mean and std\n",
    "train_data = (train_data-train_mean)/train_std\n",
    "test_data = (test_data-train_mean)/train_std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the class of the Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a Fully Connected Neural Network Architeture Class \n",
    "class OurNetwork(nn.Module):\n",
    "    def __init__(self, input_size: int, hidden_layer_size: int, output_size: int):\n",
    "        super(OurNetwork, self).__init__()\n",
    "        # define the network fully connected layers\n",
    "        self.fc_layer1 = nn.Linear(input_size, hidden_layer_size)\n",
    "        self.fc_layer2 = nn.Linear(hidden_layer_size, output_size)\n",
    "        # define a flatten layer\n",
    "        self.flatten = nn.Flatten()\n",
    "        # define a sigmoid (activation) layer\n",
    "        self.activation = nn.Sigmoid()\n",
    "      \n",
    "    def forward(self, x):\n",
    "        # define the input layer, operating on flattaned inputs\n",
    "        flattened_x = self.flatten(x)\n",
    "        # define the first layer, using linear operation and then activation\n",
    "        z1 = self.fc_layer1(flattened_x)\n",
    "        z2 = self.activation(z1)\n",
    "        # define the output layer\n",
    "        return self.fc_layer2(z2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define some hyper-parameters that will be used during the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining hyper-parameters\n",
    "hparams = Hyper_Params()\n",
    "hparams.train_size = train_size\n",
    "hparams.lr = 0.001 \n",
    "hparams.batch_size = 1000\n",
    "hparams.epochs = 30"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up the model, and define Gradient Descend as optimizer and Cross Entropy for loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the network model with a hidden layer of size 200 and send to device\n",
    "model = OurNetwork(input_size=784, hidden_layer_size=200, output_size=10).to(device)\n",
    "\n",
    "# Define the optimizer\n",
    "optimizer = optim.SGD(model.parameters(), lr=hparams.lr)\n",
    "# Define the loss criterion\n",
    "loss_function = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start a progress graph and a performance table, for visualization of the trainig process:\n",
    "hparams.fig, (hparams.ax1, hparams.ax2) = plt.subplots(2, 1, figsize=(15, 9))\n",
    "print_performance_grid(Flag=True)\n",
    "# Calculate how many iterations the model trains in each epoch\n",
    "iter_num = int(np.ceil(hparams.train_size/hparams.batch_size))\n",
    "\n",
    "# Set the model to training mode\n",
    "model.train()\n",
    "start_time = time.time() # time the start of training\n",
    "# Training loop:\n",
    "for epoch in range(hparams.epochs):\n",
    "    # for each epoch, do:\n",
    "    hparams.epoch_accuracy_train = np.zeros(iter_num)\n",
    "    hparams.epoch_loss_train = np.zeros(iter_num)\n",
    "    # randomly reshuffle the training and test groups before each new epoch:\n",
    "    index = torch.randperm(hparams.train_size)\n",
    "    train_data_perm = train_data[index]\n",
    "    train_labels_perm = train_labels[index]\n",
    "    # for each batch, do:\n",
    "    for i, batch in enumerate(range(0, hparams.train_size, hparams.batch_size)):\n",
    "        # Get a new batch of images and labels\n",
    "        data = train_data_perm[batch:batch+hparams.batch_size].to(device) \n",
    "        target = train_labels_perm[batch:batch+hparams.batch_size].squeeze().to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        output = model(data) # Apply the network on the new examples\n",
    "        loss = loss_function(output, target) # calculate the value of the loss function\n",
    "        \n",
    "        # Backward pass - ALWAYS IN THIS ORDER!\n",
    "        optimizer.zero_grad() # First, delete the gradients from the previous iteration\n",
    "        loss.backward() # Run backward pass on the loss\n",
    "        optimizer.step() # Preform an algorithm step (using the optimizer)\n",
    "\n",
    "        # Save the loss and accuracy for the graph visualization\n",
    "        hparams.epoch_accuracy_train[i] = multi_class_accuracy(output, target.squeeze().to(device))\n",
    "        hparams.epoch_loss_train[i] = loss.item()\n",
    "        if(i == 0 and epoch == 0) or ((i+1) == iter_num):\n",
    "            # Freeze the model in order to evaluate the loss and accuracy on the test set\n",
    "            model.eval()\n",
    "            test_out = model(test_data)\n",
    "            test_loss = loss_function(test_out, test_labels.squeeze()).item()\n",
    "            test_accuracy = multi_class_accuracy(test_out, test_labels.squeeze())\n",
    "            print_performance(epoch, i, hparams, test_loss, test_accuracy)\n",
    "            model.train()\n",
    "                      \n",
    "plt.show()\n",
    "print(f\"Training runs with {device}\")\n",
    "print(f\"Total training took {time.time() - start_time:.2f} seconds\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
